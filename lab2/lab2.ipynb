{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from rdkit import Chem, ML\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.ML import Scoring\n",
    "from rdkit.ML.Scoring import Scoring\n",
    "from rdkit.ML.Scoring.Scoring import CalcAUC, CalcBEDROC, CalcEnrichment\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pd.options.display.max_columns = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Give an overview of the dataset you have chosen to use.\n",
    ">\n",
    ">    - What is the classification task and what is the format of the feature data. Is this multi-task, multi-modal, or both? Explain.\n",
    ">    - Who collected the data? Why? When?\n",
    ">    - What evaluation criteria will you be using and why? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MySQL Query used to create the \"raw\" data from chemBL - note that we do filter to IC50 standard type and nM for our standard units as our only transformations prior to bringing this data into a Python Environment.  We do this to limit the size of the data ingested.  Additionally, we select on choice columns from our first table (Activities) as these columns we believed to be most important and all other columns to be irrelevant or redundant.\n",
    "```\n",
    "SELECT  a.activity_id,\n",
    "\t\ta.assay_id,\n",
    "        a.doc_id,\n",
    "        a.record_id,\n",
    "        a.molregno,\n",
    "        a.standard_relation,\n",
    "        a.standard_value,\n",
    "        a.standard_units,\n",
    "        a.standard_flag,\n",
    "        a.standard_type,\n",
    "        a.pchembl_value,\n",
    "        b.*,\n",
    "        c.*,\n",
    "        d.*,\n",
    "        e.canonical_smiles\n",
    "-- output data\n",
    "INTO OUTFILE 'C:/ProgramData/MySQL/MySQL Server 8.0/Uploads/result.tsv'\n",
    "FIELDS TERMINATED BY '\\t' OPTIONALLY ENCLOSED BY '\"'\n",
    "LINES TERMINATED BY '\\n'\n",
    "FROM chembl.activities a\n",
    "\n",
    "-- Join on ligand table\n",
    "inner join chembl.ligand_eff b\n",
    "on a.activity_id = b.activity_id\n",
    "\n",
    "-- Join on Compound Properties\n",
    "inner join chembl.compound_properties c\n",
    "on a.molregno = c.molregno\n",
    "\n",
    "-- Join on Assays\n",
    "inner join chembl.assays d\n",
    "on a.assay_id = d.assay_id\n",
    "\n",
    "-- Join on Compound Properties for the Smiles\n",
    "inner join chembl.compound_structures e\n",
    "on a.molregno = e.molregno\n",
    "\n",
    "where a.standard_type = 'IC50' and a.standard_units = 'nM';\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "df = pd.read_csv(\"chemBL.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "562960\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>standard_value</th>\n",
       "      <th>le</th>\n",
       "      <th>target_type</th>\n",
       "      <th>pref_name</th>\n",
       "      <th>canonical_smiles</th>\n",
       "      <th>molecular_species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17000.0</td>\n",
       "      <td>0.20</td>\n",
       "      <td>SINGLE PROTEIN</td>\n",
       "      <td>Palmitoyl-CoA oxidase</td>\n",
       "      <td>Cc1nc2cc(OC[C@H](O)CN3CCN(CC(=O)NCCc4ccccc4)CC...</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>180.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>SINGLE PROTEIN</td>\n",
       "      <td>Palmitoyl-CoA oxidase</td>\n",
       "      <td>Cc1nc2cc(OC[C@H](O)CN3CCN(CC(=O)Nc4cccc(-c5ccc...</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29.4</td>\n",
       "      <td>0.43</td>\n",
       "      <td>SINGLE PROTEIN</td>\n",
       "      <td>Beta-1 adrenergic receptor</td>\n",
       "      <td>CC(C)(C)NC[C@H](O)CON=C1c2ccccc2-c2ccccc21</td>\n",
       "      <td>BASE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30.8</td>\n",
       "      <td>0.43</td>\n",
       "      <td>SINGLE PROTEIN</td>\n",
       "      <td>Beta-2 adrenergic receptor</td>\n",
       "      <td>CC(C)(C)NC[C@H](O)CON=C1c2ccccc2-c2ccccc21</td>\n",
       "      <td>BASE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>120.0</td>\n",
       "      <td>0.56</td>\n",
       "      <td>PROTEIN COMPLEX GROUP</td>\n",
       "      <td>GABA-A receptor; anion channel</td>\n",
       "      <td>CCOC(=O)c1cn2c(n1)sc1ccccc12</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   standard_value    le            target_type  \\\n",
       "0         17000.0  0.20         SINGLE PROTEIN   \n",
       "1           180.0  0.25         SINGLE PROTEIN   \n",
       "2            29.4  0.43         SINGLE PROTEIN   \n",
       "3            30.8  0.43         SINGLE PROTEIN   \n",
       "4           120.0  0.56  PROTEIN COMPLEX GROUP   \n",
       "\n",
       "                        pref_name  \\\n",
       "0           Palmitoyl-CoA oxidase   \n",
       "1           Palmitoyl-CoA oxidase   \n",
       "2      Beta-1 adrenergic receptor   \n",
       "3      Beta-2 adrenergic receptor   \n",
       "4  GABA-A receptor; anion channel   \n",
       "\n",
       "                                    canonical_smiles molecular_species  \n",
       "0  Cc1nc2cc(OC[C@H](O)CN3CCN(CC(=O)NCCc4ccccc4)CC...           NEUTRAL  \n",
       "1  Cc1nc2cc(OC[C@H](O)CN3CCN(CC(=O)Nc4cccc(-c5ccc...           NEUTRAL  \n",
       "2         CC(C)(C)NC[C@H](O)CON=C1c2ccccc2-c2ccccc21              BASE  \n",
       "3         CC(C)(C)NC[C@H](O)CON=C1c2ccccc2-c2ccccc21              BASE  \n",
       "4                       CCOC(=O)c1cn2c(n1)sc1ccccc12           NEUTRAL  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print length and show sample data\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binarize ligand values based on ranges <=300 nM and >=10000 nM (1 for active, 0 for inactive respectively)\n",
    "df.loc[df[\"standard_value\"] <= 300.0, \"standard_value_bin\"] = 1\n",
    "df.loc[df[\"standard_value\"] >= 10000.0, \"standard_value_bin\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out where molecular species is null as molecular species is either base, neutral, or acidic\n",
    "df = df[~df[\"molecular_species\"].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the assay count by target name\n",
    "targets = df.groupby(\"pref_name\")[[\"target_type\"]].agg({\"target_type\": \"count\"}).sort_values(by=\"target_type\", ascending=False)\n",
    "\n",
    "targets[\"cumulative_count\"] = targets[\"target_type\"].cumsum()\n",
    "\n",
    "targets[\"cumulative_perc\"] = targets[\"cumulative_count\"]/sum(targets[\"target_type\"])\n",
    "\n",
    "targets = targets[targets[\"cumulative_perc\"] <= 0.8]\n",
    "\n",
    "targets.iloc[:100]\n",
    "\n",
    "df = df[df[\"pref_name\"].isin(targets.iloc[:100].index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHATGPT GENERATED CODE\n",
    "# Define a function to generate fingerprints\n",
    "def generate_fingerprint(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    fp = AllChem.GetMorganFingerprintAsBitVect(mol, 3, nBits=2048)\n",
    "    features = np.zeros((1,))\n",
    "    Chem.DataStructs.ConvertToNumpyArray(fp, features)\n",
    "    return features\n",
    "\n",
    "# def generate_maccs(smiles):\n",
    "#     mol = Chem.MolFromSmiles(smiles)\n",
    "#     fp = MACCSkeys.GenMACCSKeys(mol)\n",
    "#     features = list(fp.GetOnBits())\n",
    "#     return features\n",
    "\n",
    "# Apply the function to each row of the DataFrame\n",
    "df['Fingerprint'] = df[\"canonical_smiles\"].apply(generate_fingerprint)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">[1.0 points] How many tasks or modalities are there in the dataset and how do you define each task or modality? That is, explain if the task is within the same domain, cross domains, etc. If there are too many tasks or modalities to train the data reasonably, select a subset of the tasks for classification. For example, you might want to only train on 50 of the classification tasks.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([33750., 26692., 26618., 19508., 21575., 30528., 26761., 22310.,\n",
       "        23085., 19142.]),\n",
       " array([ 0. ,  9.9, 19.8, 29.7, 39.6, 49.5, 59.4, 69.3, 79.2, 89.1, 99. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot out each set of tasks\n",
    "fig, axs = plt.subplots(1, 2, sharey=True, tight_layout=True)\n",
    "\n",
    "axs[0].hist(df[\"target_type\"])\n",
    "axs[1].hist(df[\"pref_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_X_list = []\n",
    "task_y_list = []\n",
    "\n",
    "for s in df[\"pref_name\"].unique():\n",
    "    # example: neutral_X = np.array(df[df[\"molecular_species\"] == \"NEUTRAL\"][\"Fingerprint\"].tolist())\n",
    "    X = np.array(df[df[\"pref_name\"] == s][\"Fingerprint\"].tolist())\n",
    "    y = df[df[\"pref_name\"] == s][\"standard_value_bin\"].values\n",
    "\n",
    "    task_X_list.append((s, X))\n",
    "    task_y_list.append((s, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used in case want to concat other datapoints\n",
    "# np.concatenate((neutral_arr, df[df[\"molecular_species\"] == \"NEUTRAL\"][\"standard_value_bin\"].values.reshape(-1,1)), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">[1.0 points] Split the data into training and testing. Be sure to explain how you performed this operation and why you think it is reasonable to split this particular dataset this way. For multi-task datasets, be sure to explain if it is appropriate to stratify within each task. If the dataset is already split for you, explain how the split was achieved and how it is stratified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our dataset has ~250,000 records, we decided to split our data into 80% training, 20% testing. For the generalized model, we need to combine all of the tasks into a single task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in zip(task_X_list, task_y_list):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x[1], \n",
    "                                                        y[1], \n",
    "                                                        test_size=0.2, \n",
    "                                                        random_state=7)\n",
    "    all_data = {\"data\": [], \"target\": [], \"data_test\": [], \"target_test\": []}\n",
    "    all_data[\"data\"].append(X_train)\n",
    "    all_data[\"target\"].append(y_train)        \n",
    "    all_data[\"data_test\"].append(X_test)\n",
    "    all_data[\"target_test\"].append(y_test)\n",
    "\n",
    "X_train_all = np.concatenate(all_data[\"data\"])\n",
    "y_train_all = np.concatenate(all_data[\"target\"])\n",
    "\n",
    "X_test_all = np.concatenate(all_data[\"data_test\"])\n",
    "y_test_all = np.concatenate(all_data[\"target_test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">[3.0 points] Train a general model (or per task model) to perform the classification tasks. That is, a general model uses all modalities and all tasks should combined into a single classification task (if possible). Alternatively, if this is not possible, you could create a model for each specific task. For a task specific model, each task would be classified with its own feed-forward model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error as sk_mean_squared_error\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import precision_score, mean_squared_log_error\n",
    "\n",
    "def bland_altman_plot(data1, data2, *args, **kwargs):\n",
    "    mean      = np.mean([data1, data2], axis=0)\n",
    "    diff      = data1 - data2                   # Difference between data1 and data2\n",
    "    md        = np.mean(diff)                   # Mean of the difference\n",
    "    sd        = np.std(diff, axis=0)            # Standard deviation of the difference\n",
    "    \n",
    "    plt.figure(figsize=(15,10))\n",
    "    plt.scatter(mean, diff, *args, **kwargs)\n",
    "    plt.axhline(md,           color='gray', linestyle='--')\n",
    "    plt.axhline(md + 1.96*sd, color='gray', linestyle='--')\n",
    "    plt.axhline(md - 1.96*sd, color='gray', linestyle='--')\n",
    "    plt.title(\"Bland Altman, MSE: \"+str(sk_mean_squared_error(data1,data2)))\n",
    "    plt.xlabel('Mean Score', fontsize=8)\n",
    "    plt.ylabel('Diff Score', fontsize=8)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1203"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "38/38 [==============================] - 1s 12ms/step - loss: 1.6485 - accuracy: 0.0490 - auc: 0.5107 - val_loss: 1.5795 - val_accuracy: 0.0332 - val_auc: 0.5149\n",
      "Epoch 2/200\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 1.4896 - accuracy: 0.0490 - auc: 0.5382 - val_loss: 1.4367 - val_accuracy: 0.0332 - val_auc: 0.5277\n",
      "Epoch 3/200\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 1.3533 - accuracy: 0.0490 - auc: 0.5623 - val_loss: 1.3072 - val_accuracy: 0.0332 - val_auc: 0.5448\n",
      "Epoch 4/200\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 1.2302 - accuracy: 0.0490 - auc: 0.5905 - val_loss: 1.1888 - val_accuracy: 0.0365 - val_auc: 0.5593\n",
      "Epoch 5/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.1190 - accuracy: 0.0549 - auc: 0.6115 - val_loss: 1.0802 - val_accuracy: 0.0432 - val_auc: 0.5677\n",
      "Epoch 6/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.0173 - accuracy: 0.0756 - auc: 0.6298 - val_loss: 0.9802 - val_accuracy: 0.0565 - val_auc: 0.5716\n",
      "Epoch 7/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.9242 - accuracy: 0.1288 - auc: 0.6485 - val_loss: 0.8891 - val_accuracy: 0.1163 - val_auc: 0.5723\n",
      "Epoch 8/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.8389 - accuracy: 0.2186 - auc: 0.6616 - val_loss: 0.8062 - val_accuracy: 0.3090 - val_auc: 0.5806\n",
      "Epoch 9/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.7610 - accuracy: 0.3674 - auc: 0.6715 - val_loss: 0.7304 - val_accuracy: 0.5050 - val_auc: 0.5938\n",
      "Epoch 10/200\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6896 - accuracy: 0.5445 - auc: 0.6819 - val_loss: 0.6609 - val_accuracy: 0.6346 - val_auc: 0.6067\n",
      "Epoch 11/200\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6245 - accuracy: 0.7165 - auc: 0.6942 - val_loss: 0.5966 - val_accuracy: 0.7342 - val_auc: 0.6258\n",
      "Epoch 12/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.5655 - accuracy: 0.8204 - auc: 0.7053 - val_loss: 0.5381 - val_accuracy: 0.8206 - val_auc: 0.6357\n",
      "Epoch 13/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.5123 - accuracy: 0.8869 - auc: 0.7159 - val_loss: 0.4853 - val_accuracy: 0.9169 - val_auc: 0.6454\n",
      "Epoch 14/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.4648 - accuracy: 0.9152 - auc: 0.7272 - val_loss: 0.4381 - val_accuracy: 0.9435 - val_auc: 0.6572\n",
      "Epoch 15/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.4229 - accuracy: 0.9302 - auc: 0.7396 - val_loss: 0.3970 - val_accuracy: 0.9502 - val_auc: 0.6651\n",
      "Epoch 16/200\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.3864 - accuracy: 0.9401 - auc: 0.7500 - val_loss: 0.3612 - val_accuracy: 0.9568 - val_auc: 0.6703\n",
      "Epoch 17/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.3547 - accuracy: 0.9435 - auc: 0.7588 - val_loss: 0.3301 - val_accuracy: 0.9601 - val_auc: 0.6823\n",
      "Epoch 18/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.3273 - accuracy: 0.9501 - auc: 0.7670 - val_loss: 0.3031 - val_accuracy: 0.9668 - val_auc: 0.6902\n",
      "Epoch 19/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.3037 - accuracy: 0.9501 - auc: 0.7738 - val_loss: 0.2798 - val_accuracy: 0.9668 - val_auc: 0.6993\n",
      "Epoch 20/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.2835 - accuracy: 0.9501 - auc: 0.7784 - val_loss: 0.2596 - val_accuracy: 0.9668 - val_auc: 0.7024\n",
      "Epoch 21/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.2660 - accuracy: 0.9501 - auc: 0.7848 - val_loss: 0.2422 - val_accuracy: 0.9668 - val_auc: 0.7112\n",
      "Epoch 22/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.2511 - accuracy: 0.9510 - auc: 0.7900 - val_loss: 0.2273 - val_accuracy: 0.9668 - val_auc: 0.7237\n",
      "Epoch 23/200\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.2383 - accuracy: 0.9510 - auc: 0.7926 - val_loss: 0.2143 - val_accuracy: 0.9668 - val_auc: 0.7251\n",
      "Epoch 24/200\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.2274 - accuracy: 0.9510 - auc: 0.7968 - val_loss: 0.2032 - val_accuracy: 0.9668 - val_auc: 0.7222\n",
      "Epoch 25/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.2180 - accuracy: 0.9510 - auc: 0.7986 - val_loss: 0.1936 - val_accuracy: 0.9668 - val_auc: 0.7326\n",
      "Epoch 26/200\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.2100 - accuracy: 0.9510 - auc: 0.8025 - val_loss: 0.1854 - val_accuracy: 0.9668 - val_auc: 0.7333\n",
      "Epoch 27/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.2032 - accuracy: 0.9510 - auc: 0.8055 - val_loss: 0.1784 - val_accuracy: 0.9668 - val_auc: 0.7337\n",
      "Epoch 28/200\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.1974 - accuracy: 0.9510 - auc: 0.8095 - val_loss: 0.1724 - val_accuracy: 0.9668 - val_auc: 0.7402\n",
      "Epoch 29/200\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.1925 - accuracy: 0.9510 - auc: 0.8125 - val_loss: 0.1673 - val_accuracy: 0.9668 - val_auc: 0.7342\n",
      "Epoch 30/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1883 - accuracy: 0.9510 - auc: 0.8148 - val_loss: 0.1629 - val_accuracy: 0.9668 - val_auc: 0.7356\n",
      "Epoch 31/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1847 - accuracy: 0.9510 - auc: 0.8145 - val_loss: 0.1592 - val_accuracy: 0.9668 - val_auc: 0.7273\n",
      "Epoch 32/200\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.1817 - accuracy: 0.9510 - auc: 0.8188 - val_loss: 0.1560 - val_accuracy: 0.9668 - val_auc: 0.7299\n",
      "Epoch 33/200\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.1791 - accuracy: 0.9510 - auc: 0.8223 - val_loss: 0.1532 - val_accuracy: 0.9668 - val_auc: 0.7405\n",
      "Epoch 34/200\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.1769 - accuracy: 0.9510 - auc: 0.8211 - val_loss: 0.1509 - val_accuracy: 0.9668 - val_auc: 0.7371\n",
      "Epoch 35/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1749 - accuracy: 0.9510 - auc: 0.8269 - val_loss: 0.1489 - val_accuracy: 0.9668 - val_auc: 0.7364\n",
      "Epoch 36/200\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.1733 - accuracy: 0.9510 - auc: 0.8271 - val_loss: 0.1472 - val_accuracy: 0.9668 - val_auc: 0.7368\n",
      "Epoch 37/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1718 - accuracy: 0.9510 - auc: 0.8305 - val_loss: 0.1457 - val_accuracy: 0.9668 - val_auc: 0.7275\n",
      "Epoch 38/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1705 - accuracy: 0.9510 - auc: 0.8300 - val_loss: 0.1444 - val_accuracy: 0.9668 - val_auc: 0.7416\n",
      "Epoch 39/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1693 - accuracy: 0.9510 - auc: 0.8316 - val_loss: 0.1433 - val_accuracy: 0.9668 - val_auc: 0.7466\n",
      "Epoch 40/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1683 - accuracy: 0.9510 - auc: 0.8386 - val_loss: 0.1424 - val_accuracy: 0.9668 - val_auc: 0.7400\n",
      "Epoch 41/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1673 - accuracy: 0.9510 - auc: 0.8383 - val_loss: 0.1415 - val_accuracy: 0.9668 - val_auc: 0.7369\n",
      "Epoch 42/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1664 - accuracy: 0.9510 - auc: 0.8378 - val_loss: 0.1408 - val_accuracy: 0.9668 - val_auc: 0.7445\n",
      "Epoch 43/200\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.1656 - accuracy: 0.9510 - auc: 0.8392 - val_loss: 0.1401 - val_accuracy: 0.9668 - val_auc: 0.7400\n",
      "Epoch 44/200\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.1648 - accuracy: 0.9510 - auc: 0.8460 - val_loss: 0.1395 - val_accuracy: 0.9668 - val_auc: 0.7512\n",
      "Epoch 45/200\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.1641 - accuracy: 0.9510 - auc: 0.8482 - val_loss: 0.1390 - val_accuracy: 0.9668 - val_auc: 0.7438\n",
      "Epoch 46/200\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.1634 - accuracy: 0.9510 - auc: 0.8473 - val_loss: 0.1385 - val_accuracy: 0.9668 - val_auc: 0.7502\n",
      "Epoch 47/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1627 - accuracy: 0.9510 - auc: 0.8494 - val_loss: 0.1381 - val_accuracy: 0.9668 - val_auc: 0.7552\n",
      "Epoch 48/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1621 - accuracy: 0.9510 - auc: 0.8495 - val_loss: 0.1377 - val_accuracy: 0.9668 - val_auc: 0.7591\n",
      "Epoch 49/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1614 - accuracy: 0.9510 - auc: 0.8479 - val_loss: 0.1374 - val_accuracy: 0.9668 - val_auc: 0.7490\n",
      "Epoch 50/200\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.1608 - accuracy: 0.9510 - auc: 0.8522 - val_loss: 0.1371 - val_accuracy: 0.9668 - val_auc: 0.7435\n",
      "Epoch 51/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1602 - accuracy: 0.9510 - auc: 0.8530 - val_loss: 0.1368 - val_accuracy: 0.9668 - val_auc: 0.7478\n",
      "Epoch 52/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1596 - accuracy: 0.9510 - auc: 0.8573 - val_loss: 0.1365 - val_accuracy: 0.9668 - val_auc: 0.7356\n",
      "Epoch 53/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1590 - accuracy: 0.9510 - auc: 0.8597 - val_loss: 0.1363 - val_accuracy: 0.9668 - val_auc: 0.7390\n",
      "Epoch 54/200\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.1584 - accuracy: 0.9510 - auc: 0.8610 - val_loss: 0.1360 - val_accuracy: 0.9668 - val_auc: 0.7459\n",
      "Epoch 55/200\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.1579 - accuracy: 0.9510 - auc: 0.8644 - val_loss: 0.1358 - val_accuracy: 0.9668 - val_auc: 0.7485\n",
      "Epoch 56/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1573 - accuracy: 0.9510 - auc: 0.8645 - val_loss: 0.1356 - val_accuracy: 0.9668 - val_auc: 0.7522\n",
      "Epoch 57/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1568 - accuracy: 0.9510 - auc: 0.8649 - val_loss: 0.1354 - val_accuracy: 0.9668 - val_auc: 0.7553\n",
      "Epoch 58/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1562 - accuracy: 0.9510 - auc: 0.8676 - val_loss: 0.1353 - val_accuracy: 0.9668 - val_auc: 0.7577\n",
      "Epoch 59/200\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.1557 - accuracy: 0.9510 - auc: 0.8699 - val_loss: 0.1351 - val_accuracy: 0.9668 - val_auc: 0.7625\n",
      "Epoch 60/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1551 - accuracy: 0.9510 - auc: 0.8729 - val_loss: 0.1350 - val_accuracy: 0.9668 - val_auc: 0.7658\n",
      "Epoch 61/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1546 - accuracy: 0.9510 - auc: 0.8748 - val_loss: 0.1348 - val_accuracy: 0.9668 - val_auc: 0.7684\n",
      "Epoch 62/200\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.1540 - accuracy: 0.9510 - auc: 0.8772 - val_loss: 0.1347 - val_accuracy: 0.9668 - val_auc: 0.7718\n",
      "Epoch 63/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1535 - accuracy: 0.9510 - auc: 0.8793 - val_loss: 0.1346 - val_accuracy: 0.9668 - val_auc: 0.7730\n",
      "Epoch 64/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1530 - accuracy: 0.9510 - auc: 0.8769 - val_loss: 0.1344 - val_accuracy: 0.9668 - val_auc: 0.7766\n",
      "Epoch 65/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1524 - accuracy: 0.9510 - auc: 0.8749 - val_loss: 0.1343 - val_accuracy: 0.9668 - val_auc: 0.7598\n",
      "Epoch 66/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1519 - accuracy: 0.9510 - auc: 0.8769 - val_loss: 0.1342 - val_accuracy: 0.9668 - val_auc: 0.7612\n",
      "Epoch 67/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1514 - accuracy: 0.9510 - auc: 0.8783 - val_loss: 0.1341 - val_accuracy: 0.9668 - val_auc: 0.7643\n",
      "Epoch 68/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1509 - accuracy: 0.9510 - auc: 0.8767 - val_loss: 0.1340 - val_accuracy: 0.9668 - val_auc: 0.7660\n",
      "Epoch 69/200\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.1504 - accuracy: 0.9510 - auc: 0.8784 - val_loss: 0.1339 - val_accuracy: 0.9668 - val_auc: 0.7500\n",
      "Epoch 70/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1499 - accuracy: 0.9510 - auc: 0.8799 - val_loss: 0.1338 - val_accuracy: 0.9668 - val_auc: 0.7512\n",
      "Epoch 71/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1494 - accuracy: 0.9510 - auc: 0.8775 - val_loss: 0.1338 - val_accuracy: 0.9668 - val_auc: 0.7541\n",
      "Epoch 72/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1489 - accuracy: 0.9510 - auc: 0.8789 - val_loss: 0.1337 - val_accuracy: 0.9668 - val_auc: 0.7450\n",
      "Epoch 73/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1484 - accuracy: 0.9510 - auc: 0.8804 - val_loss: 0.1336 - val_accuracy: 0.9668 - val_auc: 0.7467\n",
      "Epoch 74/200\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.1479 - accuracy: 0.9510 - auc: 0.8812 - val_loss: 0.1335 - val_accuracy: 0.9668 - val_auc: 0.7491\n",
      "Epoch 75/200\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.1474 - accuracy: 0.9510 - auc: 0.8833 - val_loss: 0.1335 - val_accuracy: 0.9668 - val_auc: 0.7512\n",
      "Epoch 76/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1469 - accuracy: 0.9510 - auc: 0.8843 - val_loss: 0.1334 - val_accuracy: 0.9668 - val_auc: 0.7536\n",
      "Epoch 77/200\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.1465 - accuracy: 0.9510 - auc: 0.8855 - val_loss: 0.1334 - val_accuracy: 0.9668 - val_auc: 0.7550\n",
      "Epoch 78/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1460 - accuracy: 0.9510 - auc: 0.8865 - val_loss: 0.1333 - val_accuracy: 0.9668 - val_auc: 0.7555\n",
      "Epoch 79/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1455 - accuracy: 0.9510 - auc: 0.8874 - val_loss: 0.1333 - val_accuracy: 0.9668 - val_auc: 0.7569\n",
      "Epoch 80/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1450 - accuracy: 0.9510 - auc: 0.8882 - val_loss: 0.1333 - val_accuracy: 0.9668 - val_auc: 0.7603\n",
      "Epoch 81/200\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.1446 - accuracy: 0.9510 - auc: 0.8893 - val_loss: 0.1332 - val_accuracy: 0.9668 - val_auc: 0.7627\n",
      "Epoch 82/200\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.1441 - accuracy: 0.9510 - auc: 0.8905 - val_loss: 0.1332 - val_accuracy: 0.9668 - val_auc: 0.7637\n",
      "Epoch 83/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1437 - accuracy: 0.9510 - auc: 0.8917 - val_loss: 0.1332 - val_accuracy: 0.9668 - val_auc: 0.7663\n",
      "Epoch 84/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1432 - accuracy: 0.9510 - auc: 0.8928 - val_loss: 0.1331 - val_accuracy: 0.9668 - val_auc: 0.7686\n",
      "Epoch 85/200\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.1427 - accuracy: 0.9510 - auc: 0.8935 - val_loss: 0.1331 - val_accuracy: 0.9668 - val_auc: 0.7698\n",
      "Epoch 86/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1423 - accuracy: 0.9510 - auc: 0.8947 - val_loss: 0.1331 - val_accuracy: 0.9668 - val_auc: 0.7710\n",
      "Epoch 87/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1418 - accuracy: 0.9510 - auc: 0.8961 - val_loss: 0.1331 - val_accuracy: 0.9668 - val_auc: 0.7739\n",
      "Epoch 88/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1414 - accuracy: 0.9510 - auc: 0.8972 - val_loss: 0.1331 - val_accuracy: 0.9668 - val_auc: 0.7761\n",
      "Epoch 89/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1410 - accuracy: 0.9510 - auc: 0.8982 - val_loss: 0.1331 - val_accuracy: 0.9668 - val_auc: 0.7784\n",
      "Epoch 90/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1405 - accuracy: 0.9510 - auc: 0.8998 - val_loss: 0.1331 - val_accuracy: 0.9668 - val_auc: 0.7794\n",
      "Epoch 91/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1401 - accuracy: 0.9510 - auc: 0.9007 - val_loss: 0.1331 - val_accuracy: 0.9668 - val_auc: 0.7801\n",
      "Epoch 92/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1397 - accuracy: 0.9510 - auc: 0.9017 - val_loss: 0.1331 - val_accuracy: 0.9668 - val_auc: 0.7825\n",
      "Epoch 93/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1392 - accuracy: 0.9510 - auc: 0.9029 - val_loss: 0.1331 - val_accuracy: 0.9668 - val_auc: 0.7835\n",
      "Epoch 94/200\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.1388 - accuracy: 0.9510 - auc: 0.9037 - val_loss: 0.1331 - val_accuracy: 0.9668 - val_auc: 0.7840\n",
      "Epoch 95/200\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.1384 - accuracy: 0.9510 - auc: 0.9049 - val_loss: 0.1331 - val_accuracy: 0.9668 - val_auc: 0.7851\n",
      "Epoch 96/200\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.1380 - accuracy: 0.9510 - auc: 0.9060 - val_loss: 0.1331 - val_accuracy: 0.9668 - val_auc: 0.7871\n",
      "Epoch 97/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1376 - accuracy: 0.9510 - auc: 0.9067 - val_loss: 0.1331 - val_accuracy: 0.9668 - val_auc: 0.7912\n",
      "Epoch 98/200\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.1372 - accuracy: 0.9510 - auc: 0.9077 - val_loss: 0.1331 - val_accuracy: 0.9668 - val_auc: 0.7936\n",
      "Epoch 99/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1368 - accuracy: 0.9510 - auc: 0.9097 - val_loss: 0.1331 - val_accuracy: 0.9668 - val_auc: 0.7957\n",
      "Epoch 100/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1364 - accuracy: 0.9510 - auc: 0.9113 - val_loss: 0.1331 - val_accuracy: 0.9668 - val_auc: 0.7978\n",
      "Epoch 101/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1360 - accuracy: 0.9510 - auc: 0.9119 - val_loss: 0.1332 - val_accuracy: 0.9668 - val_auc: 0.7995\n",
      "Epoch 102/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1356 - accuracy: 0.9510 - auc: 0.9126 - val_loss: 0.1332 - val_accuracy: 0.9668 - val_auc: 0.8000\n",
      "Epoch 103/200\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.1352 - accuracy: 0.9510 - auc: 0.9129 - val_loss: 0.1332 - val_accuracy: 0.9668 - val_auc: 0.8003\n",
      "Epoch 104/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1348 - accuracy: 0.9510 - auc: 0.9135 - val_loss: 0.1332 - val_accuracy: 0.9668 - val_auc: 0.8029\n",
      "Epoch 105/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1345 - accuracy: 0.9510 - auc: 0.9142 - val_loss: 0.1333 - val_accuracy: 0.9668 - val_auc: 0.8036\n",
      "Epoch 106/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1341 - accuracy: 0.9510 - auc: 0.9150 - val_loss: 0.1333 - val_accuracy: 0.9668 - val_auc: 0.7816\n",
      "Epoch 107/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1337 - accuracy: 0.9510 - auc: 0.9154 - val_loss: 0.1333 - val_accuracy: 0.9668 - val_auc: 0.7820\n",
      "Epoch 108/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1333 - accuracy: 0.9510 - auc: 0.9160 - val_loss: 0.1334 - val_accuracy: 0.9668 - val_auc: 0.7828\n",
      "Epoch 109/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1329 - accuracy: 0.9510 - auc: 0.9172 - val_loss: 0.1334 - val_accuracy: 0.9668 - val_auc: 0.7844\n",
      "Epoch 110/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1326 - accuracy: 0.9510 - auc: 0.9181 - val_loss: 0.1334 - val_accuracy: 0.9668 - val_auc: 0.7863\n",
      "Epoch 111/200\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.1322 - accuracy: 0.9510 - auc: 0.9190 - val_loss: 0.1335 - val_accuracy: 0.9668 - val_auc: 0.7875\n",
      "Epoch 112/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1319 - accuracy: 0.9510 - auc: 0.9197 - val_loss: 0.1335 - val_accuracy: 0.9668 - val_auc: 0.7893\n",
      "Epoch 113/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1315 - accuracy: 0.9510 - auc: 0.9206 - val_loss: 0.1335 - val_accuracy: 0.9668 - val_auc: 0.7663\n",
      "Epoch 114/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1311 - accuracy: 0.9510 - auc: 0.9215 - val_loss: 0.1336 - val_accuracy: 0.9668 - val_auc: 0.7677\n",
      "Epoch 115/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1308 - accuracy: 0.9510 - auc: 0.9224 - val_loss: 0.1336 - val_accuracy: 0.9668 - val_auc: 0.7691\n",
      "Epoch 116/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1304 - accuracy: 0.9510 - auc: 0.9230 - val_loss: 0.1337 - val_accuracy: 0.9668 - val_auc: 0.7704\n",
      "Epoch 117/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1301 - accuracy: 0.9510 - auc: 0.9234 - val_loss: 0.1337 - val_accuracy: 0.9668 - val_auc: 0.7716\n",
      "Epoch 118/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1297 - accuracy: 0.9510 - auc: 0.9239 - val_loss: 0.1338 - val_accuracy: 0.9668 - val_auc: 0.7741\n",
      "Epoch 119/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1294 - accuracy: 0.9510 - auc: 0.9247 - val_loss: 0.1338 - val_accuracy: 0.9668 - val_auc: 0.7770\n",
      "Epoch 120/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1291 - accuracy: 0.9510 - auc: 0.9253 - val_loss: 0.1339 - val_accuracy: 0.9668 - val_auc: 0.7782\n",
      "Epoch 121/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1287 - accuracy: 0.9510 - auc: 0.9263 - val_loss: 0.1339 - val_accuracy: 0.9668 - val_auc: 0.7794\n",
      "Epoch 122/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1284 - accuracy: 0.9510 - auc: 0.9266 - val_loss: 0.1340 - val_accuracy: 0.9668 - val_auc: 0.7814\n",
      "Epoch 123/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1280 - accuracy: 0.9510 - auc: 0.9271 - val_loss: 0.1340 - val_accuracy: 0.9668 - val_auc: 0.7823\n",
      "Epoch 124/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1277 - accuracy: 0.9510 - auc: 0.9298 - val_loss: 0.1341 - val_accuracy: 0.9668 - val_auc: 0.7847\n",
      "Epoch 125/200\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.1274 - accuracy: 0.9510 - auc: 0.9308 - val_loss: 0.1341 - val_accuracy: 0.9668 - val_auc: 0.7749\n",
      "Epoch 126/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1270 - accuracy: 0.9501 - auc: 0.9314 - val_loss: 0.1342 - val_accuracy: 0.9668 - val_auc: 0.7771\n",
      "Epoch 127/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1267 - accuracy: 0.9510 - auc: 0.9320 - val_loss: 0.1343 - val_accuracy: 0.9635 - val_auc: 0.7773\n",
      "Epoch 128/200\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.1264 - accuracy: 0.9518 - auc: 0.9325 - val_loss: 0.1343 - val_accuracy: 0.9635 - val_auc: 0.7809\n",
      "Epoch 129/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1261 - accuracy: 0.9518 - auc: 0.9332 - val_loss: 0.1344 - val_accuracy: 0.9635 - val_auc: 0.7806\n",
      "Epoch 130/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1257 - accuracy: 0.9518 - auc: 0.9335 - val_loss: 0.1345 - val_accuracy: 0.9635 - val_auc: 0.7823\n",
      "Epoch 131/200\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.1254 - accuracy: 0.9518 - auc: 0.9339 - val_loss: 0.1345 - val_accuracy: 0.9635 - val_auc: 0.7833\n",
      "Epoch 132/200\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.1251 - accuracy: 0.9510 - auc: 0.9344 - val_loss: 0.1346 - val_accuracy: 0.9635 - val_auc: 0.7851\n",
      "Epoch 133/200\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.1248 - accuracy: 0.9510 - auc: 0.9351 - val_loss: 0.1347 - val_accuracy: 0.9601 - val_auc: 0.7852\n",
      "Epoch 134/200\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.1245 - accuracy: 0.9526 - auc: 0.9357 - val_loss: 0.1347 - val_accuracy: 0.9568 - val_auc: 0.7861\n",
      "Epoch 135/200\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.1241 - accuracy: 0.9534 - auc: 0.9366 - val_loss: 0.1348 - val_accuracy: 0.9568 - val_auc: 0.7878\n",
      "Epoch 136/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1238 - accuracy: 0.9534 - auc: 0.9370 - val_loss: 0.1349 - val_accuracy: 0.9568 - val_auc: 0.7881\n",
      "Epoch 137/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1235 - accuracy: 0.9534 - auc: 0.9373 - val_loss: 0.1350 - val_accuracy: 0.9568 - val_auc: 0.7893\n",
      "Epoch 138/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1232 - accuracy: 0.9534 - auc: 0.9378 - val_loss: 0.1350 - val_accuracy: 0.9568 - val_auc: 0.7900\n",
      "Epoch 139/200\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.1229 - accuracy: 0.9534 - auc: 0.9382 - val_loss: 0.1351 - val_accuracy: 0.9568 - val_auc: 0.7912\n",
      "Epoch 140/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1225 - accuracy: 0.9534 - auc: 0.9386 - val_loss: 0.1352 - val_accuracy: 0.9568 - val_auc: 0.7921\n",
      "Epoch 141/200\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.1222 - accuracy: 0.9534 - auc: 0.9389 - val_loss: 0.1353 - val_accuracy: 0.9568 - val_auc: 0.7935\n",
      "Epoch 142/200\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.1219 - accuracy: 0.9534 - auc: 0.9396 - val_loss: 0.1354 - val_accuracy: 0.9568 - val_auc: 0.7947\n",
      "Epoch 143/200\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.1216 - accuracy: 0.9534 - auc: 0.9400 - val_loss: 0.1354 - val_accuracy: 0.9568 - val_auc: 0.7957\n",
      "Epoch 144/200\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.1213 - accuracy: 0.9534 - auc: 0.9404 - val_loss: 0.1355 - val_accuracy: 0.9568 - val_auc: 0.7979\n",
      "Epoch 145/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1210 - accuracy: 0.9534 - auc: 0.9414 - val_loss: 0.1356 - val_accuracy: 0.9568 - val_auc: 0.7988\n",
      "Epoch 146/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1207 - accuracy: 0.9543 - auc: 0.9419 - val_loss: 0.1357 - val_accuracy: 0.9568 - val_auc: 0.7991\n",
      "Epoch 147/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1204 - accuracy: 0.9551 - auc: 0.9438 - val_loss: 0.1358 - val_accuracy: 0.9568 - val_auc: 0.8007\n",
      "Epoch 148/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1201 - accuracy: 0.9551 - auc: 0.9440 - val_loss: 0.1359 - val_accuracy: 0.9568 - val_auc: 0.8017\n",
      "Epoch 149/200\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.1198 - accuracy: 0.9551 - auc: 0.9446 - val_loss: 0.1359 - val_accuracy: 0.9568 - val_auc: 0.8024\n",
      "Epoch 150/200\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.1195 - accuracy: 0.9551 - auc: 0.9450 - val_loss: 0.1360 - val_accuracy: 0.9568 - val_auc: 0.8038\n",
      "Epoch 151/200\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.1192 - accuracy: 0.9551 - auc: 0.9456 - val_loss: 0.1361 - val_accuracy: 0.9568 - val_auc: 0.8043\n",
      "Epoch 152/200\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.1189 - accuracy: 0.9551 - auc: 0.9460 - val_loss: 0.1362 - val_accuracy: 0.9568 - val_auc: 0.8053\n",
      "Epoch 153/200\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.1186 - accuracy: 0.9551 - auc: 0.9463 - val_loss: 0.1363 - val_accuracy: 0.9568 - val_auc: 0.8060\n",
      "Epoch 154/200\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.1183 - accuracy: 0.9559 - auc: 0.9470 - val_loss: 0.1364 - val_accuracy: 0.9568 - val_auc: 0.8070\n",
      "Epoch 155/200\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.1180 - accuracy: 0.9559 - auc: 0.9474 - val_loss: 0.1365 - val_accuracy: 0.9568 - val_auc: 0.8088\n",
      "Epoch 156/200\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.1177 - accuracy: 0.9559 - auc: 0.9477 - val_loss: 0.1366 - val_accuracy: 0.9568 - val_auc: 0.8101\n",
      "Epoch 157/200\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.1174 - accuracy: 0.9568 - auc: 0.9485 - val_loss: 0.1367 - val_accuracy: 0.9568 - val_auc: 0.8112\n",
      "Epoch 158/200\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.1171 - accuracy: 0.9568 - auc: 0.9487 - val_loss: 0.1368 - val_accuracy: 0.9568 - val_auc: 0.8113\n",
      "Epoch 159/200\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.1168 - accuracy: 0.9568 - auc: 0.9492 - val_loss: 0.1369 - val_accuracy: 0.9568 - val_auc: 0.8127\n",
      "Epoch 160/200\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.1165 - accuracy: 0.9568 - auc: 0.9496 - val_loss: 0.1370 - val_accuracy: 0.9568 - val_auc: 0.8134\n",
      "Epoch 161/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1162 - accuracy: 0.9568 - auc: 0.9499 - val_loss: 0.1371 - val_accuracy: 0.9568 - val_auc: 0.8151\n",
      "Epoch 162/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1159 - accuracy: 0.9568 - auc: 0.9502 - val_loss: 0.1372 - val_accuracy: 0.9568 - val_auc: 0.8158\n",
      "Epoch 163/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1156 - accuracy: 0.9568 - auc: 0.9506 - val_loss: 0.1373 - val_accuracy: 0.9568 - val_auc: 0.8168\n",
      "Epoch 164/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1153 - accuracy: 0.9568 - auc: 0.9535 - val_loss: 0.1374 - val_accuracy: 0.9568 - val_auc: 0.8174\n",
      "Epoch 165/200\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.1150 - accuracy: 0.9568 - auc: 0.9535 - val_loss: 0.1375 - val_accuracy: 0.9568 - val_auc: 0.7911\n",
      "Epoch 166/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1147 - accuracy: 0.9568 - auc: 0.9536 - val_loss: 0.1376 - val_accuracy: 0.9568 - val_auc: 0.7938\n",
      "Epoch 167/200\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.1144 - accuracy: 0.9568 - auc: 0.9541 - val_loss: 0.1377 - val_accuracy: 0.9568 - val_auc: 0.7945\n",
      "Epoch 168/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1141 - accuracy: 0.9568 - auc: 0.9544 - val_loss: 0.1378 - val_accuracy: 0.9568 - val_auc: 0.7959\n",
      "Epoch 169/200\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.1138 - accuracy: 0.9576 - auc: 0.9545 - val_loss: 0.1379 - val_accuracy: 0.9568 - val_auc: 0.7971\n",
      "Epoch 170/200\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.1135 - accuracy: 0.9576 - auc: 0.9549 - val_loss: 0.1380 - val_accuracy: 0.9568 - val_auc: 0.7985\n",
      "Epoch 171/200\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.1133 - accuracy: 0.9576 - auc: 0.9558 - val_loss: 0.1381 - val_accuracy: 0.9568 - val_auc: 0.7983\n",
      "Epoch 172/200\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.1130 - accuracy: 0.9576 - auc: 0.9562 - val_loss: 0.1383 - val_accuracy: 0.9568 - val_auc: 0.7998\n",
      "Epoch 173/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1127 - accuracy: 0.9576 - auc: 0.9564 - val_loss: 0.1384 - val_accuracy: 0.9568 - val_auc: 0.8007\n",
      "Epoch 174/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1124 - accuracy: 0.9584 - auc: 0.9568 - val_loss: 0.1385 - val_accuracy: 0.9535 - val_auc: 0.8021\n",
      "Epoch 175/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1121 - accuracy: 0.9584 - auc: 0.9571 - val_loss: 0.1386 - val_accuracy: 0.9535 - val_auc: 0.8027\n",
      "Epoch 176/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1118 - accuracy: 0.9584 - auc: 0.9576 - val_loss: 0.1388 - val_accuracy: 0.9535 - val_auc: 0.8043\n",
      "Epoch 177/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1115 - accuracy: 0.9584 - auc: 0.9581 - val_loss: 0.1389 - val_accuracy: 0.9535 - val_auc: 0.8052\n",
      "Epoch 178/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1112 - accuracy: 0.9593 - auc: 0.9584 - val_loss: 0.1390 - val_accuracy: 0.9535 - val_auc: 0.8064\n",
      "Epoch 179/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1109 - accuracy: 0.9593 - auc: 0.9584 - val_loss: 0.1391 - val_accuracy: 0.9535 - val_auc: 0.8070\n",
      "Epoch 180/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1106 - accuracy: 0.9601 - auc: 0.9593 - val_loss: 0.1393 - val_accuracy: 0.9502 - val_auc: 0.8079\n",
      "Epoch 181/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1103 - accuracy: 0.9601 - auc: 0.9595 - val_loss: 0.1394 - val_accuracy: 0.9502 - val_auc: 0.8084\n",
      "Epoch 182/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1100 - accuracy: 0.9601 - auc: 0.9597 - val_loss: 0.1395 - val_accuracy: 0.9502 - val_auc: 0.8101\n",
      "Epoch 183/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1097 - accuracy: 0.9601 - auc: 0.9599 - val_loss: 0.1397 - val_accuracy: 0.9502 - val_auc: 0.8108\n",
      "Epoch 184/200\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.1094 - accuracy: 0.9601 - auc: 0.9602 - val_loss: 0.1398 - val_accuracy: 0.9502 - val_auc: 0.8113\n",
      "Epoch 185/200\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.1091 - accuracy: 0.9601 - auc: 0.9602 - val_loss: 0.1399 - val_accuracy: 0.9502 - val_auc: 0.8124\n",
      "Epoch 186/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1088 - accuracy: 0.9609 - auc: 0.9604 - val_loss: 0.1401 - val_accuracy: 0.9502 - val_auc: 0.8136\n",
      "Epoch 187/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1085 - accuracy: 0.9609 - auc: 0.9607 - val_loss: 0.1402 - val_accuracy: 0.9502 - val_auc: 0.8139\n",
      "Epoch 188/200\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.1082 - accuracy: 0.9601 - auc: 0.9608 - val_loss: 0.1404 - val_accuracy: 0.9502 - val_auc: 0.8153\n",
      "Epoch 189/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1079 - accuracy: 0.9601 - auc: 0.9612 - val_loss: 0.1405 - val_accuracy: 0.9502 - val_auc: 0.8156\n",
      "Epoch 190/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1076 - accuracy: 0.9601 - auc: 0.9613 - val_loss: 0.1406 - val_accuracy: 0.9502 - val_auc: 0.8165\n",
      "Epoch 191/200\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.1073 - accuracy: 0.9601 - auc: 0.9614 - val_loss: 0.1408 - val_accuracy: 0.9502 - val_auc: 0.8160\n",
      "Epoch 192/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1070 - accuracy: 0.9601 - auc: 0.9618 - val_loss: 0.1409 - val_accuracy: 0.9502 - val_auc: 0.8158\n",
      "Epoch 193/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1067 - accuracy: 0.9601 - auc: 0.9622 - val_loss: 0.1410 - val_accuracy: 0.9502 - val_auc: 0.8165\n",
      "Epoch 194/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1064 - accuracy: 0.9601 - auc: 0.9624 - val_loss: 0.1412 - val_accuracy: 0.9502 - val_auc: 0.8179\n",
      "Epoch 195/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1061 - accuracy: 0.9601 - auc: 0.9633 - val_loss: 0.1413 - val_accuracy: 0.9502 - val_auc: 0.8182\n",
      "Epoch 196/200\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.1058 - accuracy: 0.9601 - auc: 0.9635 - val_loss: 0.1415 - val_accuracy: 0.9502 - val_auc: 0.8182\n",
      "Epoch 197/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1055 - accuracy: 0.9601 - auc: 0.9638 - val_loss: 0.1416 - val_accuracy: 0.9502 - val_auc: 0.8199\n",
      "Epoch 198/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1052 - accuracy: 0.9601 - auc: 0.9640 - val_loss: 0.1417 - val_accuracy: 0.9502 - val_auc: 0.8198\n",
      "Epoch 199/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1049 - accuracy: 0.9601 - auc: 0.9643 - val_loss: 0.1419 - val_accuracy: 0.9502 - val_auc: 0.8204\n",
      "Epoch 200/200\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.1046 - accuracy: 0.9601 - auc: 0.9646 - val_loss: 0.1420 - val_accuracy: 0.9502 - val_auc: 0.8234\n"
     ]
    }
   ],
   "source": [
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=1e-6)\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=1e-6)\n",
    "\n",
    "generalized_mlp = Sequential()\n",
    "generalized_mlp.add(Dense(input_dim=X_train_all.shape[1], units=512, activation='relu', name='input_dense512'))\n",
    "generalized_mlp.add(Dense(units=256, activation='relu', name='mid_dense256'))\n",
    "generalized_mlp.add(Dense(units=256, activation='tanh', name='mid_dense2_216'))\n",
    "generalized_mlp.add(Dense(units=128, activation='relu', name='mid_dense128'))\n",
    "generalized_mlp.add(Dense(units=64, activation='tanh', name='mid_dense64'))\n",
    "generalized_mlp.add(Dense(units=32, activation='relu', name='mid_dense32'))\n",
    "generalized_mlp.add(Dense(units=16, activation='tanh', name='mid_dense16'))\n",
    "generalized_mlp.add(Dense(1, activation='tanh', name='output_layer'))\n",
    "\n",
    "generalized_mlp.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=[\"accuracy\", \"AUC\"])\n",
    "\n",
    "generalized_mlp.fit(X_train_all, y_train_all, epochs=200, shuffle=False, verbose=1, validation_data=(X_test_all,y_test_all))\n",
    "y_hat_general = generalized_mlp.predict(X_test_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_hat_converted = []\n",
    "# for i in y_hat_general:\n",
    "#     y_hat_converted.append(i[0])\n",
    "\n",
    "#don't think we need this anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def calculate_auc(y_true, y_pred):\n",
    "\n",
    "    auc = roc_auc_score(y_true, y_pred)\n",
    "    return auc\n",
    "\n",
    "auc = calculate_auc(y_true=y_test_all, y_pred=y_hat_general)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8304123711340207"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "91345feaad893ab48aac0cad8417a20c820522900c47c62ac45091529d156e55"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
